Deep Learning is a sub-field of [[🤖 Machine Learning]] that uses neural networks, which model non-linear functions using certain structures and layer types.

There are four primary model types, each serving a different purpose.
1. [[✏️ Artificial Neural Network]] for general tabular data.
2. [[👁️ Convolutional Neural Network]] for spatial data, commonly images.
3. [[💬 Recurrent Neural Network]] for sequential data, commonly text.
4. [[🤝 Graph Neural Network]] for graph data, like molecules or networks.

The following are extensions of these architectures with added benefits.
1. [[🪜 Residual Network]]s add residual connections that allow greater network depth.
2. [[🧬 Autoencoder]]s learn dimensionality reduction with encoder and decoder networks.
3. [[🕯️ Diffusion Model]]s iteratively generate new images from random noise.
4. [[🎥 Long Short Term Memory]] and [[⛩️ Gated Recurrent Unit]]s improve RNN performance for long sequences.

Using these architectures, we can build models that address specific problems.
1. [[🌞 Neural Radiance Field]] uses ANNs to create 3D scenes from multiple 2D sample images.
2. [[🍀 You Only Look Once]] and [[👟 Faster-RCNN]] tackle object detection, forming and classifying bounding boxes.
3. [[✍️ Variational Autoencoder]]s, [[💦 Normalizing Flow]], and [[🖼️ Generative Adversarial Network]]s model the dataset distribution and generate new samples.
5. [[🧵 Seq2Seq]] performs sequence translation using encoder and decoder RNNs.
6. [[🦾 Transformer]]s and [[🦿 Vision Transformer]]s combine the [[🚨 Attention Mechanism]] and networks for sequence translation.
7. [[🎊 ConvNeXt]] mimics transformers using a pure CNN architecture.