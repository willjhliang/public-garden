Deep Learning is a sub-field of [[ğŸ¤– Machine Learning]] that uses neural networks, which model non-linear functions using certain structures and layer types.

There are four primary model types, each serving a different purpose.
1. [[âœï¸ Artificial Neural Network]] for general tabular data.
2. [[ğŸ‘ï¸ Convolutional Neural Network]] for spatial data, commonly images.
3. [[ğŸ’¬ Recurrent Neural Network]] for sequential data, commonly text.
4. [[ğŸ¤ Graph Neural Network]] for graph data, like molecules or networks.

The following are extensions of these architectures with added benefits.
1. [[ğŸªœ Residual Network]]s add residual connections that allow greater network depth.
2. [[ğŸ§¬ Autoencoder]]s learn dimensionality reduction with encoder and decoder networks.
3. [[ğŸ•¯ï¸ Diffusion Model]]s iteratively generate new images from random noise.
4. [[ğŸ¥ Long Short Term Memory]] and [[â›©ï¸ Gated Recurrent Unit]]s improve RNN performance for long sequences.

Using these architectures, we can build models that address specific problems.
1. [[ğŸŒ Neural Radiance Field]] uses ANNs to create 3D scenes from multiple 2D sample images.
2. [[ğŸ€ You Only Look Once]] and [[ğŸ‘Ÿ Faster-RCNN]] tackle object detection, forming and classifying bounding boxes.
3. [[âœï¸ Variational Autoencoder]]s, [[ğŸ’¦ Normalizing Flow]], and [[ğŸ–¼ï¸ Generative Adversarial Network]]s model the dataset distribution and generate new samples.
5. [[ğŸ§µ Seq2Seq]] performs sequence translation using encoder and decoder RNNs.
6. [[ğŸ¦¾ Transformer]]s and [[ğŸ¦¿ Vision Transformer]]s combine the [[ğŸš¨ Attention Mechanism]] and networks for sequence translation.
7. [[ğŸŠ ConvNeXt]] mimics transformers using a pure CNN architecture.